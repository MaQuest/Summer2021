{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert-reco-system.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "15u3WRgp2bKVY5A4hblUzvE1X_aJurA-Q",
      "authorship_tag": "ABX9TyM5mksXeefAncEE/dep2DjQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaQuest/Summer2021/blob/main/bert_reco_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2GLIJqtDBe9S",
        "outputId": "21fa5d91-097f-4cae-fab1-b28d4b869b63"
      },
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install tweepy\n",
        "!pip install bert-extractive-summarizer\n",
        "!pip install nltk\n",
        "!pip install google-cloud-vision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/75/df441011cd1726822b70fbff50042adb4860e9327b99b346154ead704c44/sentence-transformers-1.2.0.tar.gz (81kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 4.2MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 22.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.3MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-1.2.0-cp37-none-any.whl size=123339 sha256=1df5eb8dd7da89bd71c4aecf2a079fd6cc2e5173f6e319ce631970bb87638098\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/06/f7/faaa96fdda87462b4fd5c47b343340e9d5531ef70d0eef8242\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sacremoses, huggingface-hub, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 sentence-transformers-1.2.0 sentencepiece-0.1.95 tokenizers-0.10.3 transformers-4.6.1\n",
            "Requirement already satisfied: tweepy in /usr/local/lib/python3.7/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.1)\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading https://files.pythonhosted.org/packages/1a/07/fdb05f9e18b6f641499ef56737126fbd2fafe1cdc1a04ba069d5aa205901/bert_extractive_summarizer-0.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (1.19.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers->bert-extractive-summarizer) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Installing collected packages: bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.7.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Collecting google-cloud-vision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/51/e6321162877a2903ba3158737b944cf582a62b7f045e22864ab56b764adc/google_cloud_vision-2.3.1-py2.py3-none-any.whl (461kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 5.2MB/s \n",
            "\u001b[?25hCollecting proto-plus>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/8a/61c5a9b9b6288f9b060b6e3d88374fc083953a29aeac7206616c2d3c9c8e/proto_plus-1.18.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /usr/local/lib/python3.7/dist-packages (from google-cloud-vision) (1.26.3)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from proto-plus>=1.15.0->google-cloud-vision) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.31.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (57.0.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.53.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.23.0)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.15.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (20.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.34.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (2.4.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.21.1->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-vision) (0.4.8)\n",
            "Installing collected packages: proto-plus, google-cloud-vision\n",
            "Successfully installed google-cloud-vision-2.3.1 proto-plus-1.18.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GC0RQ92BmAD"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from google.colab import drive\n",
        "import tweepy\n",
        "import re\n",
        "import os,io\n",
        "from google.cloud import vision\n",
        "from google.cloud.vision_v1 import types\n",
        "import pandas as pd\n",
        "import nltk\n",
        "\n",
        "ACCESS_TOKEN=\"1401579178072231936-98mY1wOw1UR3GHOjdo4ePnfdfUHt6n\"\n",
        "ACESS_TOKEN_SECRET=\"QiIslicgx8GNGrGDPrx8xNZc2WoCyrUsZumgXSyGzGLOx\"\n",
        "CONSUMER_KEY=\"uaiStuv7EYdYxWSomPKHvSSF5\"\n",
        "CONSUMER_SECRET=\"JOQryy37w9HPMSrSw8msyyb048iqeHmK4xCRyWP1oBhLKwLYlb\"\n",
        "\n",
        "auth=tweepy.OAuthHandler(CONSUMER_KEY,CONSUMER_SECRET)\n",
        "auth.set_access_token(ACCESS_TOKEN,ACESS_TOKEN_SECRET)\n",
        "api=tweepy.API(auth)\n",
        "\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'seismic-diorama-316110-5569927e0d86.json'\n",
        "client = vision.ImageAnnotatorClient()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o4F0RJhxPqt"
      },
      "source": [
        "def pre_process(tweets):\n",
        "\n",
        "    for i in range(0, len(tweets)):\n",
        "\n",
        "        if (tweets[i] is not None):\n",
        "\n",
        "            if(tweets[i]!=tweets[i]):\n",
        "                tweets[i]=\"\"\n",
        "\n",
        "            tweets[i] = tweets[i].lower()  # To lower case\n",
        "            tweets[i] = tweets[i].replace('@','')  # remove @\n",
        "            tweets[i] = tweets[i].replace('#','')  # remove #\n",
        "            tweets[i] = remove_urls(tweets[i])  # remove URL\n",
        "            tweets[i] = remove_emojis(tweets[i])  # remove emojis\n",
        "            tweets[i] = \"\".join(j for j in tweets[i] if j not in (\n",
        "            \"?\", \".\", \";\", \":\", \"!\", \"-\", \",\", \"[\", \"]\", \"(\", \")\", \"’\", \"‘\", '\"', \"$\", \"'\", \"“\", \"”\", \"•\", \"=\", \"+\",\n",
        "            \"%\", \"/\", \"&\", \"|\", \"~\"))  # remove punctuations\n",
        "            tweets[i] = removeNonEnglishWordsFunct(tweets[i])\n",
        "\n",
        "    return tweets\n",
        "\n",
        "def remove_urls (str):\n",
        "\n",
        "    str = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', str, flags=re.MULTILINE)\n",
        "    return(str)\n",
        "\n",
        "\n",
        "def remove_emojis(data):\n",
        "\n",
        "    emoji = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\" \n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoji, '', data)\n",
        "\n",
        "def removeNonEnglishWordsFunct(x):\n",
        "\n",
        "    new_string=re.sub('[^a-zA-Z0-9]',' ',x)\n",
        "\n",
        "    cleaned_string=re.sub('\\s+',' ',new_string)\n",
        "    return cleaned_string"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHsHsl0iBoIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2103352a-297d-4ff0-fd72-08f5017af724"
      },
      "source": [
        "df = pd.read_csv('ccc-organizations-2011_1.csv')\n",
        "\n",
        "charities =df.OrganizationName.values+\" \"+df.Description.values\n",
        "cities=df.City.values\n",
        "\n",
        "print(len(charities), \"Charities\")\n",
        "\n",
        "#We then load the allenai-specter model with SentenceTransformers\n",
        "model = SentenceTransformer('allenai-specter')\n",
        "\n",
        "#To encode the descriptions to a single string\n",
        "charity_texts = [charity for charity in charities]\n",
        "cities_texts=[city for city in cities]\n",
        "\n",
        "#Compute embeddings for all descriptions\n",
        "corpus_embeddings = model.encode(charity_texts, convert_to_tensor=True)\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "318 Charities\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trumwU454FLY"
      },
      "source": [
        "\n",
        "def search_papers_location(location):\n",
        "\n",
        "   results=[]\n",
        "   for i in range(0,len(cities_texts)):\n",
        "     if(cities_texts[i]==location):\n",
        "       results.append(i)\n",
        "   return results\n",
        "\n",
        "  "
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofFmaXf4JObB",
        "outputId": "a7fa9484-fcba-416f-8dbc-f50eae3b4287"
      },
      "source": [
        "import random\n",
        "from termcolor import colored\n",
        "\n",
        "#Generate 5 random numbers between 10 and 30\n",
        "priority = random.sample(range(0, 319), 40)\n",
        "print(priority)\n"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[122, 117, 254, 238, 233, 10, 291, 48, 157, 267, 162, 252, 80, 35, 108, 242, 137, 283, 220, 240, 276, 140, 226, 116, 310, 245, 85, 78, 75, 210, 79, 96, 318, 294, 239, 31, 270, 180, 160, 223]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBtEwFvoJe40",
        "outputId": "1a49b875-1f19-4983-a0fa-9b2941c96a1e"
      },
      "source": [
        "def generate_priority(title):\n",
        "\n",
        "   query_embedding = model.encode(title+'[SEP]', convert_to_tensor=True) # Converts to tensor\n",
        "   search_hits = util.semantic_search(query_embedding, corpus_embeddings,top_k=10)\n",
        "   search_hits = search_hits[0]\n",
        "   \n",
        "   count = 0\n",
        "   top_header = \"\\n\\nPriority Charities\\n\\n\"\n",
        "   print(colored(top_header,'yellow'))\n",
        "\n",
        "   flag=0\n",
        "\n",
        "   for hit in search_hits:\n",
        "\n",
        "          for i in range(0,len(priority)):\n",
        "            if(hit['corpus_id']==priority[i]):\n",
        "              flag=1\n",
        "              break\n",
        "\n",
        "          if(flag==1 and search_hits[count]['score']>0.60):\n",
        "            related_charities = charities[hit['corpus_id']]\n",
        "            count += 1\n",
        "            subsetDataFrame = df[df['OrganizationName']+\" \"+df['Description']== related_charities]\n",
        "            k=subsetDataFrame.values\n",
        "            print(\"\\n\"+str(count)+\") \"+colored(str(k[0][1]), 'red'))\n",
        "            print(\"similiarity score of \" + str(format(search_hits[count-1]['score'],\".2f\")))\n",
        "            print(\"Description of charity : \" +related_charities+\" \"+cities_texts[hit['corpus_id']])\n",
        "            flag=0\n",
        "            count+=1\n",
        "\n",
        "generate_priority(\"cancer\")\n"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m\n",
            "\n",
            "Priority Charities\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "1) \u001b[31mChildren's Cancer Foundation, Inc. (The)\u001b[0m\n",
            "similiarity score of 0.84\n",
            "Description of charity : Children's Cancer Foundation, Inc. (The) Provides grants for building facilities and research to hospitals in the Baltimore/Washington DC areas for pediatric oncology. Owings Mills\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8onBq7McJVg6"
      },
      "source": [
        "\n",
        "def search_papers(title,city_indices,status):\n",
        "\n",
        "   query_embedding = model.encode(title+'[SEP]', convert_to_tensor=True) # Converts to tensor\n",
        "   search_hits = util.semantic_search(query_embedding, corpus_embeddings,top_k=10)\n",
        "   search_hits = search_hits[0]\n",
        "   \n",
        "   count = 0\n",
        "   flag=0\n",
        "   \n",
        "   top_header = \"\\n\\nTop 10 related charities\\n\\n\"\n",
        "   print(colored(top_header,'blue'))\n",
        "\n",
        "\n",
        "   for hit in search_hits:\n",
        "\n",
        "        if(status==0) :\n",
        "\n",
        "          for i in range(0,len(city_indices)):\n",
        "            if(hit['corpus_id']==city_indices[i]):\n",
        "              flag=1\n",
        "              break\n",
        "\n",
        "          if(flag==1):\n",
        "\n",
        "            related_charities = charities[hit['corpus_id']]\n",
        "            count += 1\n",
        "            subsetDataFrame = df[df['OrganizationName']+\" \"+df['Description']== related_charities]\n",
        "            k=subsetDataFrame.values\n",
        "            print(\"\\n\"+str(count)+\") \"+colored(str(k[0][1]), 'red'))\n",
        "            print(\"similiarity score of \" + str(format(search_hits[count-1]['score'],\".2f\")))\n",
        "            print(\"Description of charity : \" +related_charities+\" \"+cities_texts[hit['corpus_id']])\n",
        "            flag=0\n",
        "\n",
        "        else:\n",
        "\n",
        "          related_charities = charities[hit['corpus_id']]\n",
        "          count += 1\n",
        "          subsetDataFrame = df[df['OrganizationName']+\" \"+df['Description']== related_charities]\n",
        "          k=subsetDataFrame.values\n",
        "          print(\"\\n\"+str(count)+\") \"+colored(str(k[0][1]), 'red'))\n",
        "          print(\"similiarity score of \" + str(format(search_hits[count-1]['score'],\".2f\")))\n",
        "          print(\"Description of charity : \" +related_charities+\" \"+cities_texts[hit['corpus_id']])\n",
        "          flag=0\n",
        "\n",
        "\n",
        "#want to make a big impact donations made through birdies4kids for the alberta diabetes foundation are matched up to 50  birdies for kids runs until august 15th 2021  to donate and read more \n",
        "\n",
        "#alberta wins reddeer sisters donate land to nature conservancy of canada ncc preserve a haven for species wildlife ruth dorothy bower donate 193 hectares of land on the west bank of the reddeer river bowerwildlifesanctuary\n",
        "\n",
        "#in the news the kidney foundation encourages other canadian organizations and companies to to adopt policies to support living organdonors read more about kidneycanada s wage replacement policy for living organ and tissue donation\n",
        "\n",
        "#participate in the scotiabank calgary marathon charity challenge to help children amp families at childrens cottage society theres an event for everyone of all abilities register or donate at  thanks for your consideration children families \n",
        "\n",
        "#can you help us many children need their very own books strong reading role models amp safe fun reading spaces your donation helps more children experience the magic of reading amp a lifetime of opportunity "
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNG7tMsW1gM6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "596e67b6-7c58-4130-c48d-96c89c41a096"
      },
      "source": [
        "print(\"What basis do you want to generate a recommendation on ? \")\n",
        "print(\"\\n1. Enter a sentence  \")\n",
        "print(\"2. Trending tweet on a particular hashtag \")\n",
        "print(\"3. Image-based recommendation generation \\n\")\n",
        "choice=input(\"Enter your choice : \")\n",
        "\n",
        "if(choice==\"1\"):\n",
        "\n",
        "  title=input(\"\\nEnter a sentence : \")\n",
        "  enable_loc=str(input(\"Do you wish to enable location ? \"))\n",
        "  if(enable_loc==\"yes\"):\n",
        "    status=0\n",
        "    location=str(input(\"Enter location : \"))\n",
        "    loc_indices=search_papers_location(location)\n",
        "    generate_priority(title)\n",
        "    search_papers(title,loc_indices,status)\n",
        "  else:\n",
        "    temp=[]\n",
        "    status=1\n",
        "    generate_priority(title)\n",
        "    search_papers(title,temp,status)\n",
        "\n",
        "elif(choice==\"2\"):\n",
        "\n",
        "  tag = input(\"\\nEnter a hashtag : \")\n",
        "  tweets=tweepy.Cursor(api.search,q=tag,result_type='popular',tweet_mode=\"extended\").items(1)\n",
        "  temp=[]\n",
        "  for tweet in tweets :\n",
        "\n",
        "      temp.append(tweet.full_text)\n",
        "\n",
        "  temp=pre_process(temp)\n",
        "  tweet=temp[0]\n",
        "  print(\"\\nMost famous tweet : \\n\"+colored(tweet,'red'))\n",
        "  search_papers(tweet)\n",
        "\n",
        "elif(choice==\"3\"):\n",
        "\n",
        "  with io.open(\"horse.jpg\", 'rb') as image_file:\n",
        "    content = image_file.read()\n",
        "\n",
        "\n",
        "  sample_tweet = \"\"\"Beautiful Rose (R) and beautiful Sunflower (L) are seen here enjoying a breezy afternoon here at the ranch. \n",
        "\n",
        "  Sparkleshttp://linktr.ee/PRRHR\n",
        "\n",
        "  #miniturehorses #horse #rescue #minihorsesoftwitter #BestFriendsDay #donate #tuesdayvibe\"\"\"\n",
        "\n",
        "  sample_tweet=pre_process([sample_tweet])\n",
        "\n",
        "  image = vision.Image(content=content)\n",
        "\n",
        "  response_label = client.label_detection(image=image)\n",
        "  response_text = client.text_detection(image=image)\n",
        "\n",
        "  temp=\"\"\n",
        "  count=0\n",
        "\n",
        "  for label in response_label.label_annotations:\n",
        "      if(count<3):\n",
        "        temp+=label.description+\" \"\n",
        "        count+=1\n",
        "\n",
        "  for r in response_text.text_annotations:\n",
        "      temp+=r.description+\" \"\n",
        "\n",
        "\n",
        "  temp=pre_process([temp])\n",
        "  print(\"\\n\\nInterpretation of tweet : \"+colored(sample_tweet[0]+\" \"+temp[0],'red'))\n",
        "  search_papers(sample_tweet[0]+\" \"+temp[0])"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What basis do you want to generate a recommendation on ? \n",
            "\n",
            "1. Enter a sentence  \n",
            "2. Trending tweet on a particular hashtag \n",
            "3. Image-based recommendation generation \n",
            "\n",
            "Enter your choice : 1\n",
            "\n",
            "Enter a sentence : alberta wins reddeer sisters donate land to nature conservancy of canada ncc preserve a haven for species wildlife ruth dorothy bower donate 193 hectares of land on the west bank of the reddeer river bowerwildlifesanctuary\n",
            "Do you wish to enable location ? no\n",
            "\u001b[33m\n",
            "\n",
            "Priority Charities\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "1) \u001b[31mSierra Club Foundation\u001b[0m\n",
            "similiarity score of 0.90\n",
            "Description of charity : Sierra Club Foundation Preserves wilderness and protects environmental quality worldwide through a powerful combination of education, scientific research, publishing and litigation. San Francisco\n",
            "\u001b[34m\n",
            "\n",
            "Top 10 related charities\n",
            "\n",
            "\u001b[0m\n",
            "\n",
            "1) \u001b[31mNatural Resources Defense Council\u001b[0m\n",
            "similiarity score of 0.90\n",
            "Description of charity : Natural Resources Defense Council Defends embattled wilderness, rivers, clean air, coasts and wildlife across America and around the glove. Saved Baja's whales, Canada's spirit bears and California's sequoias. New York\n",
            "\n",
            "2) \u001b[31mNature Conservancy (The)\u001b[0m\n",
            "similiarity score of 0.89\n",
            "Description of charity : Nature Conservancy (The) Working to protect ecologically important lands and waters for nature and people. Over 119 million acres of land and 5000 miles of water protected worldwide. Arlington\n",
            "\n",
            "3) \u001b[31mConservation Fund (The)\u001b[0m\n",
            "similiarity score of 0.87\n",
            "Description of charity : Conservation Fund (The) Nation's most effective and efficient conservation organization; protecting working landscapes, wildlife habitat, and historic and recreation sites with 97% of funds going directly to the mission. Arlington\n",
            "\n",
            "4) \u001b[31mEarthShare\u001b[0m\n",
            "similiarity score of 0.86\n",
            "Description of charity : EarthShare Donations are shared among our member groups to protect our environment, health, wildlife and natural resources. One environment, one simple way to care for it. Bethesda\n",
            "\n",
            "5) \u001b[31mSierra Club Foundation\u001b[0m\n",
            "similiarity score of 0.86\n",
            "Description of charity : Sierra Club Foundation Preserves wilderness and protects environmental quality worldwide through a powerful combination of education, scientific research, publishing and litigation. San Francisco\n",
            "\n",
            "6) \u001b[31mMid-Atlantic Council of Trout Unlimited\u001b[0m\n",
            "similiarity score of 0.86\n",
            "Description of charity : Mid-Atlantic Council of Trout Unlimited Dedicated to the conservation, protection and restoration of coldwater trout streams and their watersheds throughout Maryland. Clarksville\n",
            "\n",
            "7) \u001b[31mAmerican Forests\u001b[0m\n",
            "similiarity score of 0.85\n",
            "Description of charity : American Forests Funds private and public land reforestation projects, provides satellite imagery of tree loss to cities and educates the public on the value of tress and forests. Washington\n",
            "\n",
            "8) \u001b[31mNational Aquarium in Baltimore\u001b[0m\n",
            "similiarity score of 0.85\n",
            "Description of charity : National Aquarium in Baltimore More than an attraction, the Aquarium provides environmental education experiences and engages citizens in conservation projects that benefit the region's watersheds and beyond. Baltimore\n",
            "\n",
            "9) \u001b[31m1000 Friends of Maryland\u001b[0m\n",
            "similiarity score of 0.85\n",
            "Description of charity : 1000 Friends of Maryland Works to protect Maryland's natural areas, restore vibrant towns and cities and improve public transportation through education, research and advocacy. Baltimore\n",
            "\n",
            "10) \u001b[31mFrisky's Wildlife and Primate Sanctuary, Inc.\u001b[0m\n",
            "similiarity score of 0.84\n",
            "Description of charity : Frisky's Wildlife and Primate Sanctuary, Inc. Rehabilitates injured, orphaned or abandoned wildlife animals. We are a sanctuary for ex-exotic pets to live out their lives. Woodstock\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}