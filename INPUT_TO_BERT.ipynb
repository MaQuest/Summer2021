{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "INPUT_TO_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1AHB-V3sp40PLGyLfMjlcFzG1RJbxjra_",
      "authorship_tag": "ABX9TyNwybVLTQ0kXMNQTBVckvzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaQuest/Summer2021/blob/main/INPUT_TO_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0euvsXRzCee"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKXS7R4-y7BE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import torch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpzQkqaUzLGu"
      },
      "source": [
        "\n",
        "MODELS = [(BertModel, BertTokenizer, 'bert-base-uncased')]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnt0-4y8zOhy"
      },
      "source": [
        "Using bert-base uncased model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V6XACS2zc_W"
      },
      "source": [
        "# LOADING OUR BERT MODEL AND TOKENIZER FROM IN-BUILT BERT \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUuoJncUzOCE",
        "outputId": "29f2202c-5aff-427b-ced8-af536cc6ab7a"
      },
      "source": [
        "for model_class, tokenizer_class, pretrained_weights in MODELS:\n",
        "    \n",
        "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "    bert_model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYsG3TINzu3z"
      },
      "source": [
        "# READING OUR SST-2 SENTIMENT BANK DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7SY0cFzptU"
      },
      "source": [
        "\n",
        "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOrXlGmbz-aI"
      },
      "source": [
        "# USING 4000 SENTENCES FOR FASTER PROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJZ_yGeqz6pl"
      },
      "source": [
        "\n",
        "batch = df[:4000]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma46bsj00INs"
      },
      "source": [
        "# TOKENIZING AND PADDING OUR DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh1pywG_0ELt"
      },
      "source": [
        "def tokenize_cut_pad(df):\n",
        "    \n",
        "    df = df.copy()\n",
        "    \n",
        "    max_input_size = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "    \n",
        "    # shorten sequences longer than BERT max input size\n",
        "    df[0] = [text[:max_input_size - 2] for text in df[0].values] \n",
        "    tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True))) # tokenizes and converts tokens to ids, includes special tokens\n",
        "    \n",
        "    max_len = 0\n",
        "    for i in tokenized.values:\n",
        "        if len(i) > max_len:\n",
        "            # max_len will be equal to longest sequence in the tokenized values\n",
        "            max_len = len(i)\n",
        "\n",
        "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "    \n",
        "    return torch.tensor(padded)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhMflhFV0cep"
      },
      "source": [
        "# Get BERT model embedding for each CLS token in each example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WOZPfoE0cN2"
      },
      "source": [
        "input_ids = tokenize_cut_pad(batch)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSe2w6iT0t6Y"
      },
      "source": [
        "with torch.no_grad():\n",
        "    last_hidden_states = bert_model(input_ids)[0]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4VWbxbS4ShZ"
      },
      "source": [
        "# STORING LAST_HIDDEN_STATE IN VARIABLE FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxzJThJM4O0g"
      },
      "source": [
        "features = last_hidden_states[:,0,:].numpy()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PqkZ-0c4aGo"
      },
      "source": [
        "# STORING THE TEST SENTIMENT WHETHER 0 OR 1 IN LABELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A_lnyUA4pkz"
      },
      "source": [
        "\n",
        "labels = batch[1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ37M0eO4uE8"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels,test_size = 0.15)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CRvrQu25C0X"
      },
      "source": [
        "# DEFAULT SPLIT TO 75-25%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OQXJPHD48su",
        "outputId": "77db7a81-cb44-4c01-cec8-f675ef36201f"
      },
      "source": [
        "\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3400, 768)\n",
            "(600, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR-LW-3N5Iaf"
      },
      "source": [
        "# INITIALIZING OUR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HxDMS2a5OHL",
        "outputId": "6aba9742-fdd5-41a0-9df4-0d6f70784a0e"
      },
      "source": [
        "\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "model.fit(train_features, train_labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo456vd_5SoH"
      },
      "source": [
        "# TESTING OUR MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAUqfG475WOY",
        "outputId": "dfd71f45-e187-4622-f1a7-bd5f14b7fd2c"
      },
      "source": [
        "model.score(test_features, test_labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8166666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYdDzLka5dEo"
      },
      "source": [
        "# PREDICTION OF MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyUI4jAc5lmf"
      },
      "source": [
        "# INPUT SENTENCE FROM USER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYqC1c5J6RiF"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/stackexchange_data.csv')\n",
        "\n",
        "sentences = df.question_text.values[:500]\n",
        "\n",
        "sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eds5hZglXCgG",
        "outputId": "8362aca3-7dfe-4ea8-c47b-c51aeb9cb270"
      },
      "source": [
        "\n",
        "def prediction(text):\n",
        "    \n",
        "    input_text = tokenizer.encode(text)\n",
        "    test_input_ids = torch.tensor(input_text)\n",
        "    test_input_ids = test_input_ids.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        hidden_states = bert_model(test_input_ids)[0]\n",
        "    test_features = hidden_states[:, 0, :].numpy()\n",
        "    pred = model.predict(test_features)[0]\n",
        "    return pred\n",
        "\n",
        "t = 0.0\n",
        "\n",
        "sentiments = []\n",
        "\n",
        "score = model.score(test_features, test_labels)\n",
        "count = 0\n",
        "for i in sentences:\n",
        "    pred = prediction(i)\n",
        "    sentiments.append(i+ \" \"+ \" sentiment score: \" + str(pred)+\" \")\n",
        "    print(i+ \" \"+ \" sentiment score: \" + str(pred)+\" \")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In Microsoft Professional, is there a way for resource task assignments to autopopulate as resource engagements for utilization reporting purposes? If so, how?  sentiment score: 0 \n",
            "If I have to formulate goals for a project there's the well established S.M.A.R.T. paradigm. It helps to have goals at the end, from which necessary actions can be easily deduced. It prevents from having goals which can be interpreted differently thus causing a lot of problems with stakeholders and sponsors.\n",
            "I wonder if there are similar paradigms for formulating risks when doing a risk analysis. I often find myself searching for risks by just mindmapping them over every topic which comes to my mind or doing that together with a team. The amount of theoretical situations and constellations posing risks is often overwhelming.\n",
            "Within an hour of a meeting we wrote down and counted 60 terminal nodes on a mindmap representing some sort of risk in different areas and points of view. I think we are not bad in \"risk identification\".\n",
            "The thing is, those \"risks\" found by mindmapping tend to overlap and are often formulated in a way that they describe sometimes the risk itself, sometimes the cause and sometimes a failed mitigation and mixed cases from all that. \n",
            "e.g. if I try to assess the risks of cleaning my bathroom (of course so I can skip that this week to reduce overall risk) I find a lot of opportunities where i can stumble on slippery surfaces. I can use inapproriate detergents which are corrosive to my bathtub as well as to my skin, eyes and aspiratory system and even exacerbated by inadvertedly mixing them together. But if I want to break down the \"inappropriate detergent\" which is maybe more a cause for a risk than a risk itself I get a lot of overlapping risks, because i can harm my skin with different chemicals but those might or might not ruin my bathtub's enamel. That's what I want to name \"overlapping risks\".\n",
            "I hope you get my point. I think you can't asses risks in a reliable way if you can't formulate them well. I'd like to have some kind of a touchstone to check, if a risk is well formulated. Same as there is for goals.  sentiment score: 0 \n",
            "Does it make sense to use Scrum when the dates for sprints constantly change?\n",
            "Our team uses Azure DevOps with Scrum. We have multiple codebases to maintain. However, since things are usually rather chaotic with some team members getting constantly pulled off to work on \"other things\", the dates are constantly getting messed up.  Not only are members (usually 1 or 2) getting tasked with other things, but the end dates get pushed due to \"other priorities\" that come up that might affect the entire team.\n",
            "Options being considered:\n",
            "\n",
            "Lower capacity on that sprint for the members that constantly get pulled off, to accommodate the \"flux\" of other demands that crop up?\n",
            "Once a sprint starts and something comes up that will push the release date, then adjust the end date on the sprint iteration.\n",
            "\n",
            "In a way it makes me wonder if we simply need to use a spreadsheet to keep track of \"stuff to do\" and tag one field with a \"release #\" for those updates getting into the release build. That is a management call however.  If we continue with using Scrum, there is no way that any dates, analytics could ever be accurate, could they?  Azure DevOps then becomes used as simply a Git repo and an over-complicated \"task\" list?  sentiment score: 0 \n",
            "I am using MS Project to calculate the Earn Value of one task. let's call it Mech Installation. The task has a duration of 30 days (from 2/6/20 to 3/18/20). In the Resource Sheet I added 'Mechanical', of type Cost.\n",
            "I have saved the baseline, set the status date to be 2/18/20, and finally Mech Installation has a percent complete equals to 25%. Here is where I am not sure what is happening: when I switch to Earn Value Table I get a Plan Value (PV) equal to 0, an Earn Value (EV) equal to $3,750, and a Schedule Variance has a value of -$750.00. This makes me think that MS Project is calculating a value for PV but it is not displaying it.\n",
            "Why is PV equal 0? Should it not be equal to $4,500? More importantly, how can I display the right value of PV?  sentiment score: 0 \n",
            "Closed. This question needs details or clarity. It is not currently accepting answers.\n",
            "                            \n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Want to improve this question? Add details and clarify the problem by editing this post.\n",
            "                        \n",
            "Closed 3 days ago.\n",
            "\n",
            "\n",
            "\n",
            "What are the responsibilities of a business analyst during a product release?  sentiment score: 0 \n",
            "Let's say we have five Stories, A, B, C, D, and E. However, while D and E are clearly separate business requirements and should be tested/documented separately, the technical requirements are related/similar, and so it would make sense for them to be worked on together at the same time by one person/pair.\n",
            "Let's also assume that Stories A, B, and C are in-progress, and the Work In Progress (WIP) limit is 4. We also have a dev/pair that just finished working on something else and now wants to work on D/E.\n",
            "What would be the best approach here?\n",
            "\n",
            "Just take both D and E into in-progress, violating WIP limits\n",
            "Take only D or E into in-progress, and thereby work less efficiently on them\n",
            "Don't take either into in-progress, and instead help to get A, B, or C out of the way asap\n",
            "Get one of the other devs to stop working on A, B, or C\n",
            "Something I've missed?  sentiment score: 0 \n",
            "As the scrum master is a mentor and guide for a team, the team trusts the scrum master to lead them on the right path to \"being\" agile. But how can a scrum master be sure of the same? Each team is different and there's no benchmark (Or is there?) to measure one's success in doing the same?\n",
            "Personally, due to this vagueness/lack of clarity in measurability, I find it difficult to decide on SMART goals for myself.  sentiment score: 0 \n",
            "Closed. This question is opinion-based. It is not currently accepting answers.\n",
            "                            \n",
            "                        \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Want to improve this question? Update the question so it can be answered with facts and citations by editing this post.\n",
            "                        \n",
            "Closed 7 days ago.\n",
            "\n",
            "\n",
            "\n",
            "What are the best lines quoted by project management experts for adopting change in a company.  sentiment score: 0 \n",
            "currently we have a client that have a series of requirements (very broad and general) for an app. I as the PM was asked by my boss to estimate how much hours of work were required to make that app. Now, given the requirements I can see this taking between two and two and a half months, but since this estimated hours are going to be use to charge the client I need to go into details to give a proper estimation. \n",
            "The problem is that my boss want this ASAP, 1-2 two days (big surprise), but of course I'll need to review this carefully with my team in order to give a proper estimate, and there is always the posibility that the client don't like the proposal and declines the offer, therefore we will lose a lot of time. My question is how should I balance the time invested to make an estimation? How much minimun time should I invest? Should I talk to my boss and explained to him why I need more time? Or should I give my glanced estimations of two and a half months?\n",
            "I write down the requirements and transform them into user stories, etc, but since we need to work with a third party we do not know for sure how their system (ERP) works, that is the reason why I believe that a lot of unforeseen problems my arise.  sentiment score: 0 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-0ea5e91f1648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0msentiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" sentiment score: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\" sentiment score: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-0ea5e91f1648>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtest_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m         )\n\u001b[1;32m    971\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"absolute\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (540) must match the size of tensor b (512) at non-singleton dimension 1"
          ]
        }
      ]
    }
  ]
}